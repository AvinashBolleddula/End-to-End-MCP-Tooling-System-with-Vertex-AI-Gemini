# Build Your Own MCP Server & Client (STDIO + Gemini)

This repository demonstrates how to **build a complete Model Context Protocol (MCP) system from scratch**, including:

- An **MCP server** exposing tools using FastMCP
- An **MCP client** that launches the server and communicates via **JSON-RPC 2.0 over STDIO**
- Integration with **Vertex AI Gemini** for reasoning and tool orchestration
- A correct **multi-step tool chaining loop** (not limited to single-round tool calls)

This project is **production-aligned, and fully runnable locally**.

---
## ğŸ—ï¸ Architecture Diagram

**Important distinction**

- Terminal I/O â†’ **human interaction**
- STDIO transport â†’ **machine-to-machine JSON-RPC communication**
  
```mermaid
flowchart LR
    User["ğŸ‘¤ User<br/>(CLI)"]

    subgraph Client["ğŸ§  MCP Client"]
        Gemini["âœ¨ Gemini (Vertex AI)<br/>Reasoning Engine"]
        Session["ğŸ”Œ MCP ClientSession<br/>(JSON-RPC)"]
    end

    subgraph Server["ğŸ›  MCP Server"]
        FastMCP["âš™ï¸ FastMCP Runtime"]
        Tools["ğŸŒ¦ Weather Tools<br/>â€¢ get_forecast<br/>â€¢ get_alerts"]
    end

    User -->|"Query / Response"| Client
    Gemini -->|"Tool calls"| Session
    Session -->|"JSON-RPC 2.0"| FastMCP
    FastMCP --> Tools
```
---
## ğŸ—ï¸ Execution Sequence (End-to-End)
```mermaid
sequenceDiagram
    participant U as User (CLI)
    participant C as MCP Client
    participant L as Gemini (Vertex AI)
    participant S as MCP Server
    participant T as Weather Tools

    U->>C: Enter natural language query
    C->>L: Send prompt + tool schemas
    L->>C: Emit structured tool call
    C->>S: JSON-RPC call_tool request
    S->>T: Execute weather function
    T-->>S: Tool result
    S-->>C: JSON-RPC response
    C->>L: Provide tool output
    L-->>C: Final natural language response
    C-->>U: Display result

```

**Key Notes**
- STDIO is used only for **client â†” server JSON-RPC**
- Terminal I/O is strictly **human interaction**
- Gemini never calls tools directly â€” it **requests**, the client executes
- Tool chaining is handled by a **while-loop on the client**

---

## ğŸ“ Project Structure
```text
Build-Your-Own-Mcp-Server-Client/
â”œâ”€â”€ weather/
â”‚   â”œâ”€â”€ weather.py        # MCP server: FastMCP runtime + weather tools
â”‚   â”œâ”€â”€ client.py         # MCP client: Gemini reasoning + tool loop
â”‚   â”œâ”€â”€ pyproject.toml    # uv project configuration & dependencies
â”‚   â”œâ”€â”€ uv.lock           # Locked, reproducible dependency versions
â”‚   â”œâ”€â”€ .gitignore        # Ignores .env, .venv, caches, OS artifacts
â”‚   â””â”€â”€ README.md         # Weather MCP module documentation
â””â”€â”€ README.md             # Root project overview & architecture
```
---

## ğŸš€ Features

- âœ… MCP server built using **FastMCP**
- âœ… **STDIO-based** JSON-RPC communication
- âœ… Tool discovery (`list_tools`)
- âœ… Tool execution (`call_tool`)
- âœ… **Multi-round tool chaining** using a proper loop
- âœ… **Vertex AI Gemini** integration (IAM-based auth)
- âœ… Async-safe lifecycle management with `AsyncExitStack`
- âœ… Clean dependency management using **uv**

---

## ğŸ› ï¸ Prerequisites

- Python **3.10+**
- [`uv`](https://github.com/astral-sh/uv) package manager
- Google Cloud project with:
  - Vertex AI enabled
  - Gemini model access
- Authenticated locally using:
  ```bash
  gcloud auth application-default login
  ```
---
## âš™ï¸ Setup Instructions

Follow these steps to run the MCP server and client locally.

---

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/AvinashBolleddula/Build-Your-Own-Mcp-Server-Client.git
cd Build-Your-Own-Mcp-Server-Client/weather
```

### 2ï¸âƒ£ Create and activate a virtual environment
This project uses uv for fast and reproducible Python environments.
```bash
uv venv
source .venv/bin/activate
```
You should now see (.venv) in your terminal prompt.

### 3ï¸âƒ£ Install dependencies
Install all required dependencies exactly as defined in pyproject.toml and uv.lock.
```bash
uv sync
```
### 4ï¸âƒ£ Configure environment variables
Create a .env file inside the weather/ directory:
```bash
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_LOCATION=us-central1
GEMINI_MODEL=gemini-2.0-flash
```
Note
Vertex AI uses IAM authentication, not API keys
Ensure you are authenticated locally using:
```bash
gcloud auth application-default login
```
### 5ï¸âƒ£ Run the MCP client and server
From inside the weather/ directory:
```bash
python client.py weather.py
```
If everything is configured correctly, you should see:
```bash
Connected to server with tools: ['get_alerts', 'get_forecast']
MCP Client Started!
```
You can now start interacting with the system via the terminal.
